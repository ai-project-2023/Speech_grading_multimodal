{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ai-project-2023/Speech_grading_multimodal/blob/nlp%2Fspeechtotext/speechtotext_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MP4_GOOD에 있는 파일 TEST"
      ],
      "metadata": {
        "id": "wKv6MrWcKC1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6FUNmV2wo58",
        "outputId": "5f3a19f8-250f-4019-bad5-1fc15ce73927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzM7WRrkvcwG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 드라이브 폴더 경로 지정\n",
        "good_script_path = '/content/drive/MyDrive/AI_Project_14/MP4_GOOD/transcript'  # 실제 폴더명으로 수정\n",
        "\n",
        "# 폴더 내의 모든 파일 이름을 리스트로 저장\n",
        "good_script_names = os.listdir(good_script_path)\n",
        "\n",
        "print(good_script_names)\n",
        "print(len(good_script_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx977-fLxVXY",
        "outputId": "85e3bb94-256d-4431-8d2a-2ee34575b911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SherylSandberg_2010W.txt', 'YochaiBenkler_2005G.txt', 'JohnWooden_2001.txt', 'MatthieuRicard_2004.txt', 'WillardWigan_2009G.txt', 'PeterDonnelly_2005G.txt', 'JohnLloyd_2009G.txt', 'SugataMitra_2010G.txt', 'WoodyNorris_2004.txt', 'MarcPachter_2008P.txt', 'PeterEigen_2009X.txt', 'ZainabSalbi_2010G.txt', 'PeterReinhart_2008P.txt', 'MenaTrott_2006.txt', 'PatrickAwuah_2007G.txt', 'NielsDiffrient_2002a.txt', 'JohnHodgman_2008.txt', 'SherylWuDunn_2010G.txt', 'StephenPetranek_2002.txt', 'JohnDoerr_2007.txt', 'RozSavage_2010Z.txt', 'MarvinMinsky_2003.txt', 'SuheirHammad_2010W.txt', 'TimBernersLee_2010U.txt', 'LorettaNapoleoni_2009G.txt', 'ThomasGoetz_2010P.txt', 'JosephNye_2010G.txt', 'LauraTrice_2008.txt', 'SasaVucinic_2005G.txt', 'JosephPine_2004.txt', 'PhilippeStarck_2007.txt', 'MarkBittman_2007P.txt', 'JuliaSweeney_2006.txt', 'TomHoney_2005.txt', 'LesleyHazleton_2010X.txt', 'JohnFrancis_2008.txt', 'MarkRoth_2010.txt', 'SherwinNuland_2003.txt', 'KamalMeattle_2009U.txt', 'TonyPorter_2010W.txt', 'VanJones_2010X.txt', 'NicMarks_2010G.txt', 'WilliamUry_2010X.txt']\n",
            "43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 드라이브 폴더 경로 지정\n",
        "good_result_path = '/content/drive/MyDrive/AI_Project_14/MP4_GOOD/text_results'  # 실제 폴더명으로 수정\n",
        "\n",
        "# 폴더 내의 모든 파일 이름을 리스트로 저장\n",
        "good_result_names = os.listdir(good_result_path)\n",
        "\n",
        "print(good_result_names)\n",
        "print(len(good_result_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N45z1OWxvNU",
        "outputId": "2a773d57-46b2-4590-b33d-f8c017b2be9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['WillardWigan_2009G.wav.wav.txt', 'JosephNye_2010G.wav.wav.txt', 'JohnWooden_2001.wav.wav.txt', 'JohnLloyd_2009G.wav.wav.txt', 'JohnHodgman_2008.wav.wav.txt', 'JohnFrancis_2008.wav.wav.txt', 'JohnDoerr_2007.wav.wav.txt', 'ZainabSalbi_2010G.wav.wav.txt', 'YochaiBenkler_2005G.wav.wav.txt', 'WoodyNorris_2004.wav.wav.txt', 'WilliamUry_2010X.sph.wav.txt', 'LorettaNapoleoni_2009G.wav.wav.txt', 'LesleyHazleton_2010X.wav.wav.txt', 'LauraTrice_2008.wav.wav.txt', 'KamalMeattle_2009U.wav.wav.txt', 'JuliaSweeney_2006.wav.wav.txt', 'JosephPine_2004.wav.wav.txt', 'MatthieuRicard_2004.wav.wav.txt', 'MarvinMinsky_2003.wav.wav.txt', 'MarkRoth_2010.wav.wav.txt', 'MarkBittman_2007P.wav.wav.txt', 'MarcPachter_2008P.wav.wav.txt', 'PatrickAwuah_2007G.wav.wav.txt', 'NielsDiffrient_2002a.wav.wav.txt', 'NicMarks_2010G.wav.wav.txt', 'MenaTrott_2006.wav.wav.txt', 'RozSavage_2010Z.wav.wav.txt', 'PhilippeStarck_2007.wav.wav.txt', 'PeterReinhart_2008P.wav.wav.txt', 'PeterEigen_2009X.wav.wav.txt', 'PeterDonnelly_2005G.wav.wav.txt', 'StephenPetranek_2002.wav.wav.txt', 'SherylWuDunn_2010G.wav.wav.txt', 'SherylSandberg_2010W.wav.wav.txt', 'SherwinNuland_2003.wav.wav.txt', 'SasaVucinic_2005G.wav.wav.txt', 'TonyPorter_2010W.wav.wav.txt', 'TomHoney_2005.wav.wav.txt', 'TimBernersLee_2010U.wav.wav.txt', 'ThomasGoetz_2010P.wav.wav.txt', 'SuheirHammad_2010W.wav.wav.txt', 'SugataMitra_2010G.wav.wav.txt', 'VanJones_2010X.wav.wav.txt', '0000.txt', '0034.txt', '0032.txt', '0031.txt', '0033.txt', '0030.txt', '0029.txt', '0026.txt', '0028.txt', '0025.txt', '0027.txt', '0021.txt', '0022.txt', '0023.txt', '0024.txt', '0020.txt', '0015.txt', '0017.txt', '0018.txt', '0013.txt', '0012.txt', '0010.txt', '0019.txt', '0016.txt', '0014.txt', '0011.txt', '0008.txt', '0007.txt', '0009.txt', '0006.txt', '0005.txt', '0002.txt', '0004.txt', '0001.txt', '0003.txt', '0044.txt', '0043.txt', '0041.txt', '0042.txt', '0045.txt', '0040.txt', '0039.txt', '0036.txt', '0038.txt', '0037.txt', '0035.txt']\n",
            "89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_names = [name.split('.')[0] for name in good_script_names if name.split('.')[0] in [result.split('.')[0] for result in good_result_names]]\n",
        "\n",
        "print(filtered_names)\n",
        "print(len(filtered_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIj6VG5PyXSf",
        "outputId": "db6848b0-83fa-45bc-e77b-c34b6b0db88a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SherylSandberg_2010W', 'YochaiBenkler_2005G', 'JohnWooden_2001', 'MatthieuRicard_2004', 'WillardWigan_2009G', 'PeterDonnelly_2005G', 'JohnLloyd_2009G', 'SugataMitra_2010G', 'WoodyNorris_2004', 'MarcPachter_2008P', 'PeterEigen_2009X', 'ZainabSalbi_2010G', 'PeterReinhart_2008P', 'MenaTrott_2006', 'PatrickAwuah_2007G', 'NielsDiffrient_2002a', 'JohnHodgman_2008', 'SherylWuDunn_2010G', 'StephenPetranek_2002', 'JohnDoerr_2007', 'RozSavage_2010Z', 'MarvinMinsky_2003', 'SuheirHammad_2010W', 'TimBernersLee_2010U', 'LorettaNapoleoni_2009G', 'ThomasGoetz_2010P', 'JosephNye_2010G', 'LauraTrice_2008', 'SasaVucinic_2005G', 'JosephPine_2004', 'PhilippeStarck_2007', 'MarkBittman_2007P', 'JuliaSweeney_2006', 'TomHoney_2005', 'LesleyHazleton_2010X', 'JohnFrancis_2008', 'MarkRoth_2010', 'SherwinNuland_2003', 'KamalMeattle_2009U', 'TonyPorter_2010W', 'VanJones_2010X', 'NicMarks_2010G', 'WilliamUry_2010X']\n",
            "43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_info = pd.DataFrame()"
      ],
      "metadata": {
        "id": "JYpgqcUvxSFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name in filtered_names:\n",
        "    file_path = os.path.join(good_script_path, name + '.txt')\n",
        "    with open(file_path, 'r') as file:\n",
        "        script_text = file.read()\n",
        "\n",
        "    result_file_path = os.path.join(good_result_path, name + '.wav.wav.txt')\n",
        "    if os.path.exists(result_file_path):\n",
        "        with open(result_file_path, 'r') as result_file:\n",
        "            result_text = result_file.read()\n",
        "    else:\n",
        "        result_text = ''  # 결과 정보가 없는 경우 빈 문자열로 처리\n",
        "\n",
        "    test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJcT8Ft70nis",
        "outputId": "23856835-b1ca-4506-9809-9e9849f0aa6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n",
            "<ipython-input-36-e8416c071aa5>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  test_info = test_info.append({'file_name': name, 'script': script_text, 'result': result_text}, ignore_index=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z-WdJJzi1GjT",
        "outputId": "70d68cd8-4117-474a-bfe2-094f576691e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 file_name                                             script  \\\n",
              "0     SherylSandberg_2010W   a couple of years ago i was in new york and i...   \n",
              "1      YochaiBenkler_2005G  in some contexts it's even more efficient beca...   \n",
              "2          JohnWooden_2001  every shot that is taken they assumed would be...   \n",
              "3      MatthieuRicard_2004  the state of in a state where there's nothing ...   \n",
              "4       WillardWigan_2009G   the she thought i was at school and i used to...   \n",
              "5      PeterDonnelly_2005G   now that's a logical error it's exactly the s...   \n",
              "6          JohnLloyd_2009G  the late geoffrey dickens m p was attending a ...   \n",
              "7        SugataMitra_2010G  one billion children we need one hundred milli...   \n",
              "8         WoodyNorris_2004   put lenses in front of it focus it a little b...   \n",
              "9        MarcPachter_2008P  but i promise you it was the right question th...   \n",
              "10        PeterEigen_2009X  i used to work as the director of the world ba...   \n",
              "11       ZainabSalbi_2010G   and killed him and his father but did not kil...   \n",
              "12     PeterReinhart_2008P  and love for the flavor and ultimately the cha...   \n",
              "13          MenaTrott_2006  i'm a blogger which probably to a lot of you m...   \n",
              "14      PatrickAwuah_2007G   with a march of democracy and free markets ac...   \n",
              "15    NielsDiffrient_2002a  so you don't get hard spots you cant hit your ...   \n",
              "16        JohnHodgman_2008  i am not a scientist i have never built an ato...   \n",
              "17      SherylWuDunn_2010G  she still made the two hour trek to the school...   \n",
              "18    StephenPetranek_2002  so what can we do about this oh by the way we'...   \n",
              "19          JohnDoerr_2007  i can't wait to see what we tedsters do about ...   \n",
              "20         RozSavage_2010Z  but that attitude that awareness that leads yo...   \n",
              "21       MarvinMinsky_2003  the single best way is to get them to understa...   \n",
              "22      SuheirHammad_2010W  i understand i've been wrong that means is pre...   \n",
              "23     TimBernersLee_2010U  which houses are there which houses have been ...   \n",
              "24  LorettaNapoleoni_2009G  the time of the phone call i just had a baby i...   \n",
              "25       ThomasGoetz_2010P   educate them or inform them but actually lead...   \n",
              "26         JosephNye_2010G  the industrial revolution in which while prote...   \n",
              "27         LauraTrice_2008  so the question is why was i blocking it why w...   \n",
              "28       SasaVucinic_2005G  you do it you start it in the states because i...   \n",
              "29         JosephPine_2004  long distance telephone service sold on price ...   \n",
              "30     PhilippeStarck_2007  we give you white pages invent we give you the...   \n",
              "31       MarkBittman_2007P  seventy percent of the agricultural land on ea...   \n",
              "32       JuliaSweeney_2006   credit for it and worst of all how could i no...   \n",
              "33           TomHoney_2005  how could one practice such a faith by seeking...   \n",
              "34    LesleyHazleton_2010X   phrases and snippets taken out of context in ...   \n",
              "35        JohnFrancis_2008  i didn a t know who i would be but i know i ne...   \n",
              "36           MarkRoth_2010  so it's really incredible for me to be a part ...   \n",
              "37      SherwinNuland_2003  a great deal it's the old concept the real gre...   \n",
              "38      KamalMeattle_2009U  the third plant is money plant and this is aga...   \n",
              "39        TonyPorter_2010W  when they were about five and six four and fiv...   \n",
              "40          VanJones_2010X  also suffer at the point of use those of us wh...   \n",
              "41          NicMarks_2010G  a deer a deer freezes very very still  poised ...   \n",
              "42        WilliamUry_2010X  if it can be done in europe why not in the mid...   \n",
              "\n",
              "                                               result  \n",
              "0   tip for any of us in this room today let's sta...  \n",
              "1   one of the problems of writing and working and...  \n",
              "2   icon my own definition definition of success i...  \n",
              "3   so I guess it is a result of globalization tha...  \n",
              "4   there's an old saying just because you can't s...  \n",
              "5   as long as we couldn't have said it's a rather...  \n",
              "6   so question is what is invisible there's more ...  \n",
              "7   well that's kind of an obvious statement of th...  \n",
              "8   I became an inventor by accident and I was out...  \n",
              "9   the National Portrait Gallery is the place ded...  \n",
              "10  I'm going to speak about corruption but I woul...  \n",
              "11  I woke up in the middle of the night with a so...  \n",
              "12  this is a wheat bread a whole wheat bread and ...  \n",
              "13  open past couple days I've been preparing for ...  \n",
              "14  like many of you here. I am trying to contribu...  \n",
              "15  when I was 5 years old. I fell in love with ai...  \n",
              "16  he won't know this story in the summer of 1950...  \n",
              "17  yoga challenge that I want to talk to you abou...  \n",
              "18  the advances that have taken place in astronom...  \n",
              "19  I'm really scared. I don't think we're going t...  \n",
              "20  hi my name is Ron Savage and I rode across oce...  \n",
              "21  if you ask people about what what part of psyc...  \n",
              "22  but I will I will not dance to your war drum. ...  \n",
              "23  last year he said I asked you to give me at 8 ...  \n",
              "24  I'm not sure you out terrorism actually intera...  \n",
              "25  I'm going to talking to you about how we can t...  \n",
              "26  I better talk to you about the power in this t...  \n",
              "27  hi I'm here to talk to you about the importanc...  \n",
              "28  an event scene for one point of view gives one...  \n",
              "29  I'm a very fundamental change that is going on...  \n",
              "30  you will understand messing with my type of En...  \n",
              "31  I write about food I write about cooking I I t...  \n",
              "32  on September 10th the morning of my seventh bi...  \n",
              "33  I'm a Victor in the Church of England. I've be...  \n",
              "34  you may have heard about the crimes idea of pa...  \n",
              "35  thank you for being here. and I'm I say thank ...  \n",
              "36  I'm going to talk to you today about my work o...  \n",
              "37  you know I am so bad at Tech that my daughter ...  \n",
              "38  from 17 years ago I became allergic to Delisa ...  \n",
              "39  drive to update New York City between Harlem a...  \n",
              "40  I am honored to be here and I'm honored to tal...  \n",
              "41  Martin Luther King. did not say I have a night...  \n",
              "42                                                     "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8930fdce-5ee6-4968-b506-f4b5f34221b6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>script</th>\n",
              "      <th>result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SherylSandberg_2010W</td>\n",
              "      <td>a couple of years ago i was in new york and i...</td>\n",
              "      <td>tip for any of us in this room today let's sta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>YochaiBenkler_2005G</td>\n",
              "      <td>in some contexts it's even more efficient beca...</td>\n",
              "      <td>one of the problems of writing and working and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>JohnWooden_2001</td>\n",
              "      <td>every shot that is taken they assumed would be...</td>\n",
              "      <td>icon my own definition definition of success i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MatthieuRicard_2004</td>\n",
              "      <td>the state of in a state where there's nothing ...</td>\n",
              "      <td>so I guess it is a result of globalization tha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>WillardWigan_2009G</td>\n",
              "      <td>the she thought i was at school and i used to...</td>\n",
              "      <td>there's an old saying just because you can't s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>PeterDonnelly_2005G</td>\n",
              "      <td>now that's a logical error it's exactly the s...</td>\n",
              "      <td>as long as we couldn't have said it's a rather...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>JohnLloyd_2009G</td>\n",
              "      <td>the late geoffrey dickens m p was attending a ...</td>\n",
              "      <td>so question is what is invisible there's more ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>SugataMitra_2010G</td>\n",
              "      <td>one billion children we need one hundred milli...</td>\n",
              "      <td>well that's kind of an obvious statement of th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>WoodyNorris_2004</td>\n",
              "      <td>put lenses in front of it focus it a little b...</td>\n",
              "      <td>I became an inventor by accident and I was out...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>MarcPachter_2008P</td>\n",
              "      <td>but i promise you it was the right question th...</td>\n",
              "      <td>the National Portrait Gallery is the place ded...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>PeterEigen_2009X</td>\n",
              "      <td>i used to work as the director of the world ba...</td>\n",
              "      <td>I'm going to speak about corruption but I woul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>ZainabSalbi_2010G</td>\n",
              "      <td>and killed him and his father but did not kil...</td>\n",
              "      <td>I woke up in the middle of the night with a so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>PeterReinhart_2008P</td>\n",
              "      <td>and love for the flavor and ultimately the cha...</td>\n",
              "      <td>this is a wheat bread a whole wheat bread and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>MenaTrott_2006</td>\n",
              "      <td>i'm a blogger which probably to a lot of you m...</td>\n",
              "      <td>open past couple days I've been preparing for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>PatrickAwuah_2007G</td>\n",
              "      <td>with a march of democracy and free markets ac...</td>\n",
              "      <td>like many of you here. I am trying to contribu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>NielsDiffrient_2002a</td>\n",
              "      <td>so you don't get hard spots you cant hit your ...</td>\n",
              "      <td>when I was 5 years old. I fell in love with ai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>JohnHodgman_2008</td>\n",
              "      <td>i am not a scientist i have never built an ato...</td>\n",
              "      <td>he won't know this story in the summer of 1950...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>SherylWuDunn_2010G</td>\n",
              "      <td>she still made the two hour trek to the school...</td>\n",
              "      <td>yoga challenge that I want to talk to you abou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>StephenPetranek_2002</td>\n",
              "      <td>so what can we do about this oh by the way we'...</td>\n",
              "      <td>the advances that have taken place in astronom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>JohnDoerr_2007</td>\n",
              "      <td>i can't wait to see what we tedsters do about ...</td>\n",
              "      <td>I'm really scared. I don't think we're going t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>RozSavage_2010Z</td>\n",
              "      <td>but that attitude that awareness that leads yo...</td>\n",
              "      <td>hi my name is Ron Savage and I rode across oce...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>MarvinMinsky_2003</td>\n",
              "      <td>the single best way is to get them to understa...</td>\n",
              "      <td>if you ask people about what what part of psyc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>SuheirHammad_2010W</td>\n",
              "      <td>i understand i've been wrong that means is pre...</td>\n",
              "      <td>but I will I will not dance to your war drum. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>TimBernersLee_2010U</td>\n",
              "      <td>which houses are there which houses have been ...</td>\n",
              "      <td>last year he said I asked you to give me at 8 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>LorettaNapoleoni_2009G</td>\n",
              "      <td>the time of the phone call i just had a baby i...</td>\n",
              "      <td>I'm not sure you out terrorism actually intera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>ThomasGoetz_2010P</td>\n",
              "      <td>educate them or inform them but actually lead...</td>\n",
              "      <td>I'm going to talking to you about how we can t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>JosephNye_2010G</td>\n",
              "      <td>the industrial revolution in which while prote...</td>\n",
              "      <td>I better talk to you about the power in this t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>LauraTrice_2008</td>\n",
              "      <td>so the question is why was i blocking it why w...</td>\n",
              "      <td>hi I'm here to talk to you about the importanc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>SasaVucinic_2005G</td>\n",
              "      <td>you do it you start it in the states because i...</td>\n",
              "      <td>an event scene for one point of view gives one...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>JosephPine_2004</td>\n",
              "      <td>long distance telephone service sold on price ...</td>\n",
              "      <td>I'm a very fundamental change that is going on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>PhilippeStarck_2007</td>\n",
              "      <td>we give you white pages invent we give you the...</td>\n",
              "      <td>you will understand messing with my type of En...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>MarkBittman_2007P</td>\n",
              "      <td>seventy percent of the agricultural land on ea...</td>\n",
              "      <td>I write about food I write about cooking I I t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>JuliaSweeney_2006</td>\n",
              "      <td>credit for it and worst of all how could i no...</td>\n",
              "      <td>on September 10th the morning of my seventh bi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>TomHoney_2005</td>\n",
              "      <td>how could one practice such a faith by seeking...</td>\n",
              "      <td>I'm a Victor in the Church of England. I've be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>LesleyHazleton_2010X</td>\n",
              "      <td>phrases and snippets taken out of context in ...</td>\n",
              "      <td>you may have heard about the crimes idea of pa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>JohnFrancis_2008</td>\n",
              "      <td>i didn a t know who i would be but i know i ne...</td>\n",
              "      <td>thank you for being here. and I'm I say thank ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>MarkRoth_2010</td>\n",
              "      <td>so it's really incredible for me to be a part ...</td>\n",
              "      <td>I'm going to talk to you today about my work o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>SherwinNuland_2003</td>\n",
              "      <td>a great deal it's the old concept the real gre...</td>\n",
              "      <td>you know I am so bad at Tech that my daughter ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>KamalMeattle_2009U</td>\n",
              "      <td>the third plant is money plant and this is aga...</td>\n",
              "      <td>from 17 years ago I became allergic to Delisa ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>TonyPorter_2010W</td>\n",
              "      <td>when they were about five and six four and fiv...</td>\n",
              "      <td>drive to update New York City between Harlem a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>VanJones_2010X</td>\n",
              "      <td>also suffer at the point of use those of us wh...</td>\n",
              "      <td>I am honored to be here and I'm honored to tal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>NicMarks_2010G</td>\n",
              "      <td>a deer a deer freezes very very still  poised ...</td>\n",
              "      <td>Martin Luther King. did not say I have a night...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>WilliamUry_2010X</td>\n",
              "      <td>if it can be done in europe why not in the mid...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8930fdce-5ee6-4968-b506-f4b5f34221b6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8930fdce-5ee6-4968-b506-f4b5f34221b6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8930fdce-5ee6-4968-b506-f4b5f34221b6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_info = test_info[test_info['result'] != \"\"]\n",
        "test_info['cos_sim'] = 0"
      ],
      "metadata": {
        "id": "yz3OlbQl1qYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6V0KQGkM1rDP",
        "outputId": "3714a02b-d12a-47cf-9d4c-342179abc1f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 file_name                                             script  \\\n",
              "0     SherylSandberg_2010W   a couple of years ago i was in new york and i...   \n",
              "1      YochaiBenkler_2005G  in some contexts it's even more efficient beca...   \n",
              "2          JohnWooden_2001  every shot that is taken they assumed would be...   \n",
              "3      MatthieuRicard_2004  the state of in a state where there's nothing ...   \n",
              "4       WillardWigan_2009G   the she thought i was at school and i used to...   \n",
              "5      PeterDonnelly_2005G   now that's a logical error it's exactly the s...   \n",
              "6          JohnLloyd_2009G  the late geoffrey dickens m p was attending a ...   \n",
              "7        SugataMitra_2010G  one billion children we need one hundred milli...   \n",
              "8         WoodyNorris_2004   put lenses in front of it focus it a little b...   \n",
              "9        MarcPachter_2008P  but i promise you it was the right question th...   \n",
              "10        PeterEigen_2009X  i used to work as the director of the world ba...   \n",
              "11       ZainabSalbi_2010G   and killed him and his father but did not kil...   \n",
              "12     PeterReinhart_2008P  and love for the flavor and ultimately the cha...   \n",
              "13          MenaTrott_2006  i'm a blogger which probably to a lot of you m...   \n",
              "14      PatrickAwuah_2007G   with a march of democracy and free markets ac...   \n",
              "15    NielsDiffrient_2002a  so you don't get hard spots you cant hit your ...   \n",
              "16        JohnHodgman_2008  i am not a scientist i have never built an ato...   \n",
              "17      SherylWuDunn_2010G  she still made the two hour trek to the school...   \n",
              "18    StephenPetranek_2002  so what can we do about this oh by the way we'...   \n",
              "19          JohnDoerr_2007  i can't wait to see what we tedsters do about ...   \n",
              "20         RozSavage_2010Z  but that attitude that awareness that leads yo...   \n",
              "21       MarvinMinsky_2003  the single best way is to get them to understa...   \n",
              "22      SuheirHammad_2010W  i understand i've been wrong that means is pre...   \n",
              "23     TimBernersLee_2010U  which houses are there which houses have been ...   \n",
              "24  LorettaNapoleoni_2009G  the time of the phone call i just had a baby i...   \n",
              "25       ThomasGoetz_2010P   educate them or inform them but actually lead...   \n",
              "26         JosephNye_2010G  the industrial revolution in which while prote...   \n",
              "27         LauraTrice_2008  so the question is why was i blocking it why w...   \n",
              "28       SasaVucinic_2005G  you do it you start it in the states because i...   \n",
              "29         JosephPine_2004  long distance telephone service sold on price ...   \n",
              "30     PhilippeStarck_2007  we give you white pages invent we give you the...   \n",
              "31       MarkBittman_2007P  seventy percent of the agricultural land on ea...   \n",
              "32       JuliaSweeney_2006   credit for it and worst of all how could i no...   \n",
              "33           TomHoney_2005  how could one practice such a faith by seeking...   \n",
              "34    LesleyHazleton_2010X   phrases and snippets taken out of context in ...   \n",
              "35        JohnFrancis_2008  i didn a t know who i would be but i know i ne...   \n",
              "36           MarkRoth_2010  so it's really incredible for me to be a part ...   \n",
              "37      SherwinNuland_2003  a great deal it's the old concept the real gre...   \n",
              "38      KamalMeattle_2009U  the third plant is money plant and this is aga...   \n",
              "39        TonyPorter_2010W  when they were about five and six four and fiv...   \n",
              "40          VanJones_2010X  also suffer at the point of use those of us wh...   \n",
              "41          NicMarks_2010G  a deer a deer freezes very very still  poised ...   \n",
              "\n",
              "                                               result  cos_sim  \n",
              "0   tip for any of us in this room today let's sta...        0  \n",
              "1   one of the problems of writing and working and...        0  \n",
              "2   icon my own definition definition of success i...        0  \n",
              "3   so I guess it is a result of globalization tha...        0  \n",
              "4   there's an old saying just because you can't s...        0  \n",
              "5   as long as we couldn't have said it's a rather...        0  \n",
              "6   so question is what is invisible there's more ...        0  \n",
              "7   well that's kind of an obvious statement of th...        0  \n",
              "8   I became an inventor by accident and I was out...        0  \n",
              "9   the National Portrait Gallery is the place ded...        0  \n",
              "10  I'm going to speak about corruption but I woul...        0  \n",
              "11  I woke up in the middle of the night with a so...        0  \n",
              "12  this is a wheat bread a whole wheat bread and ...        0  \n",
              "13  open past couple days I've been preparing for ...        0  \n",
              "14  like many of you here. I am trying to contribu...        0  \n",
              "15  when I was 5 years old. I fell in love with ai...        0  \n",
              "16  he won't know this story in the summer of 1950...        0  \n",
              "17  yoga challenge that I want to talk to you abou...        0  \n",
              "18  the advances that have taken place in astronom...        0  \n",
              "19  I'm really scared. I don't think we're going t...        0  \n",
              "20  hi my name is Ron Savage and I rode across oce...        0  \n",
              "21  if you ask people about what what part of psyc...        0  \n",
              "22  but I will I will not dance to your war drum. ...        0  \n",
              "23  last year he said I asked you to give me at 8 ...        0  \n",
              "24  I'm not sure you out terrorism actually intera...        0  \n",
              "25  I'm going to talking to you about how we can t...        0  \n",
              "26  I better talk to you about the power in this t...        0  \n",
              "27  hi I'm here to talk to you about the importanc...        0  \n",
              "28  an event scene for one point of view gives one...        0  \n",
              "29  I'm a very fundamental change that is going on...        0  \n",
              "30  you will understand messing with my type of En...        0  \n",
              "31  I write about food I write about cooking I I t...        0  \n",
              "32  on September 10th the morning of my seventh bi...        0  \n",
              "33  I'm a Victor in the Church of England. I've be...        0  \n",
              "34  you may have heard about the crimes idea of pa...        0  \n",
              "35  thank you for being here. and I'm I say thank ...        0  \n",
              "36  I'm going to talk to you today about my work o...        0  \n",
              "37  you know I am so bad at Tech that my daughter ...        0  \n",
              "38  from 17 years ago I became allergic to Delisa ...        0  \n",
              "39  drive to update New York City between Harlem a...        0  \n",
              "40  I am honored to be here and I'm honored to tal...        0  \n",
              "41  Martin Luther King. did not say I have a night...        0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3ad37d51-b9a2-499a-a061-2eb4bb0b897b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>script</th>\n",
              "      <th>result</th>\n",
              "      <th>cos_sim</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SherylSandberg_2010W</td>\n",
              "      <td>a couple of years ago i was in new york and i...</td>\n",
              "      <td>tip for any of us in this room today let's sta...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>YochaiBenkler_2005G</td>\n",
              "      <td>in some contexts it's even more efficient beca...</td>\n",
              "      <td>one of the problems of writing and working and...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>JohnWooden_2001</td>\n",
              "      <td>every shot that is taken they assumed would be...</td>\n",
              "      <td>icon my own definition definition of success i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MatthieuRicard_2004</td>\n",
              "      <td>the state of in a state where there's nothing ...</td>\n",
              "      <td>so I guess it is a result of globalization tha...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>WillardWigan_2009G</td>\n",
              "      <td>the she thought i was at school and i used to...</td>\n",
              "      <td>there's an old saying just because you can't s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>PeterDonnelly_2005G</td>\n",
              "      <td>now that's a logical error it's exactly the s...</td>\n",
              "      <td>as long as we couldn't have said it's a rather...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>JohnLloyd_2009G</td>\n",
              "      <td>the late geoffrey dickens m p was attending a ...</td>\n",
              "      <td>so question is what is invisible there's more ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>SugataMitra_2010G</td>\n",
              "      <td>one billion children we need one hundred milli...</td>\n",
              "      <td>well that's kind of an obvious statement of th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>WoodyNorris_2004</td>\n",
              "      <td>put lenses in front of it focus it a little b...</td>\n",
              "      <td>I became an inventor by accident and I was out...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>MarcPachter_2008P</td>\n",
              "      <td>but i promise you it was the right question th...</td>\n",
              "      <td>the National Portrait Gallery is the place ded...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>PeterEigen_2009X</td>\n",
              "      <td>i used to work as the director of the world ba...</td>\n",
              "      <td>I'm going to speak about corruption but I woul...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>ZainabSalbi_2010G</td>\n",
              "      <td>and killed him and his father but did not kil...</td>\n",
              "      <td>I woke up in the middle of the night with a so...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>PeterReinhart_2008P</td>\n",
              "      <td>and love for the flavor and ultimately the cha...</td>\n",
              "      <td>this is a wheat bread a whole wheat bread and ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>MenaTrott_2006</td>\n",
              "      <td>i'm a blogger which probably to a lot of you m...</td>\n",
              "      <td>open past couple days I've been preparing for ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>PatrickAwuah_2007G</td>\n",
              "      <td>with a march of democracy and free markets ac...</td>\n",
              "      <td>like many of you here. I am trying to contribu...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>NielsDiffrient_2002a</td>\n",
              "      <td>so you don't get hard spots you cant hit your ...</td>\n",
              "      <td>when I was 5 years old. I fell in love with ai...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>JohnHodgman_2008</td>\n",
              "      <td>i am not a scientist i have never built an ato...</td>\n",
              "      <td>he won't know this story in the summer of 1950...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>SherylWuDunn_2010G</td>\n",
              "      <td>she still made the two hour trek to the school...</td>\n",
              "      <td>yoga challenge that I want to talk to you abou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>StephenPetranek_2002</td>\n",
              "      <td>so what can we do about this oh by the way we'...</td>\n",
              "      <td>the advances that have taken place in astronom...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>JohnDoerr_2007</td>\n",
              "      <td>i can't wait to see what we tedsters do about ...</td>\n",
              "      <td>I'm really scared. I don't think we're going t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>RozSavage_2010Z</td>\n",
              "      <td>but that attitude that awareness that leads yo...</td>\n",
              "      <td>hi my name is Ron Savage and I rode across oce...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>MarvinMinsky_2003</td>\n",
              "      <td>the single best way is to get them to understa...</td>\n",
              "      <td>if you ask people about what what part of psyc...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>SuheirHammad_2010W</td>\n",
              "      <td>i understand i've been wrong that means is pre...</td>\n",
              "      <td>but I will I will not dance to your war drum. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>TimBernersLee_2010U</td>\n",
              "      <td>which houses are there which houses have been ...</td>\n",
              "      <td>last year he said I asked you to give me at 8 ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>LorettaNapoleoni_2009G</td>\n",
              "      <td>the time of the phone call i just had a baby i...</td>\n",
              "      <td>I'm not sure you out terrorism actually intera...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>ThomasGoetz_2010P</td>\n",
              "      <td>educate them or inform them but actually lead...</td>\n",
              "      <td>I'm going to talking to you about how we can t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>JosephNye_2010G</td>\n",
              "      <td>the industrial revolution in which while prote...</td>\n",
              "      <td>I better talk to you about the power in this t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>LauraTrice_2008</td>\n",
              "      <td>so the question is why was i blocking it why w...</td>\n",
              "      <td>hi I'm here to talk to you about the importanc...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>SasaVucinic_2005G</td>\n",
              "      <td>you do it you start it in the states because i...</td>\n",
              "      <td>an event scene for one point of view gives one...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>JosephPine_2004</td>\n",
              "      <td>long distance telephone service sold on price ...</td>\n",
              "      <td>I'm a very fundamental change that is going on...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>PhilippeStarck_2007</td>\n",
              "      <td>we give you white pages invent we give you the...</td>\n",
              "      <td>you will understand messing with my type of En...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>MarkBittman_2007P</td>\n",
              "      <td>seventy percent of the agricultural land on ea...</td>\n",
              "      <td>I write about food I write about cooking I I t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>JuliaSweeney_2006</td>\n",
              "      <td>credit for it and worst of all how could i no...</td>\n",
              "      <td>on September 10th the morning of my seventh bi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>TomHoney_2005</td>\n",
              "      <td>how could one practice such a faith by seeking...</td>\n",
              "      <td>I'm a Victor in the Church of England. I've be...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>LesleyHazleton_2010X</td>\n",
              "      <td>phrases and snippets taken out of context in ...</td>\n",
              "      <td>you may have heard about the crimes idea of pa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>JohnFrancis_2008</td>\n",
              "      <td>i didn a t know who i would be but i know i ne...</td>\n",
              "      <td>thank you for being here. and I'm I say thank ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>MarkRoth_2010</td>\n",
              "      <td>so it's really incredible for me to be a part ...</td>\n",
              "      <td>I'm going to talk to you today about my work o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>SherwinNuland_2003</td>\n",
              "      <td>a great deal it's the old concept the real gre...</td>\n",
              "      <td>you know I am so bad at Tech that my daughter ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>KamalMeattle_2009U</td>\n",
              "      <td>the third plant is money plant and this is aga...</td>\n",
              "      <td>from 17 years ago I became allergic to Delisa ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>TonyPorter_2010W</td>\n",
              "      <td>when they were about five and six four and fiv...</td>\n",
              "      <td>drive to update New York City between Harlem a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>VanJones_2010X</td>\n",
              "      <td>also suffer at the point of use those of us wh...</td>\n",
              "      <td>I am honored to be here and I'm honored to tal...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>NicMarks_2010G</td>\n",
              "      <td>a deer a deer freezes very very still  poised ...</td>\n",
              "      <td>Martin Luther King. did not say I have a night...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ad37d51-b9a2-499a-a061-2eb4bb0b897b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3ad37d51-b9a2-499a-a061-2eb4bb0b897b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3ad37d51-b9a2-499a-a061-2eb4bb0b897b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCiGWrnq1y8h",
        "outputId": "90ec3e24-eeb3-41bc-a9e5-3df537c7fe0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "\n",
        "for i in range(test_info.shape[0]):\n",
        "\n",
        "  model_name = 'bert-base-uncased'\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModel.from_pretrained(model_name)\n",
        "  cos_sim_list = []\n",
        "  # 문장 입력\n",
        "  sentence1 = test_info.loc[i]['result']\n",
        "  sentence2 = test_info.loc[i]['script']\n",
        "\n",
        "  # 토큰화 및 임베딩 벡터 변환\n",
        "  tokens1 = tokenizer.tokenize(sentence1)\n",
        "  tokens2 = tokenizer.tokenize(sentence2)\n",
        "  input_ids1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
        "  input_ids2 = tokenizer.convert_tokens_to_ids(tokens2)\n",
        "  input_ids1_1 = input_ids1[:512]\n",
        "  input_ids2_1 = input_ids2[:512]\n",
        "\n",
        "  input_tensor1_1 = torch.tensor([input_ids1_1])\n",
        "  input_tensor2_1 = torch.tensor([input_ids2_1])\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output1_1 = model(input_tensor1_1)\n",
        "      output2_1 = model(input_tensor2_1)\n",
        "\n",
        "  sentence_embedding1_1 = torch.mean(output1_1[0], dim=1)  # 첫 번째 문장의 평균 임베딩\n",
        "  sentence_embedding2_1 = torch.mean(output2_1[0], dim=1)  # 두 번째 문장의 평균 임베딩\n",
        "\n",
        "  # cosine_similarity 계산\n",
        "  cosine_similarity_1 = torch.nn.functional.cosine_similarity(sentence_embedding1_1, sentence_embedding2_1, dim=0)\n",
        "\n",
        "  mean_value_1 = torch.mean(cosine_similarity_1)\n",
        "  cos_sim_list.append(mean_value_1)\n",
        "\n",
        "  input_ids1_2 = input_ids1[-512:]\n",
        "  input_ids2_2 = input_ids2[-512:]\n",
        "\n",
        "  input_tensor1_2 = torch.tensor([input_ids1_2])\n",
        "  input_tensor2_2 = torch.tensor([input_ids2_2])\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output1_2 = model(input_tensor1_2)\n",
        "      output2_2 = model(input_tensor2_2)\n",
        "\n",
        "  sentence_embedding1_2 = torch.mean(output1_2[0], dim=1)  # 첫 번째 문장의 평균 임베딩\n",
        "  sentence_embedding2_2 = torch.mean(output2_2[0], dim=1)  # 두 번째 문장의 평균 임베딩\n",
        "\n",
        "  # cosine_similarity 계산\n",
        "  cosine_similarity_2 = torch.nn.functional.cosine_similarity(sentence_embedding1_2, sentence_embedding2_2, dim=0)\n",
        "\n",
        "  mean_value_2 = torch.mean(cosine_similarity_2)\n",
        "  cos_sim_list.append(mean_value_2)\n",
        "  cos_sim = np.mean(cos_sim_list)\n",
        "  print(i)\n",
        "  print(cos_sim)\n",
        "  test_info['cos_sim'].iloc[i] = cos_sim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHTuVpwi12ED",
        "outputId": "5df1e2f3-754a-4ba9-aabe-451fdc95414f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3022 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0.8359375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-17b5e858392c>:60: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_info['cos_sim'].iloc[i] = cos_sim\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2899 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0.7890625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3470 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "0.77734375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2637 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "0.6432291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2710 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "0.76953125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (4173 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "0.7083334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1840 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "0.7565104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2109 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n",
            "0.73046875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2621 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "0.8072916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3644 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "0.7317709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2140 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "0.72265625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2801 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n",
            "0.6705729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3139 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n",
            "0.69661456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3425 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n",
            "0.80078125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2387 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n",
            "0.67578125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2423 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n",
            "0.76953125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2863 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "0.8190104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3162 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17\n",
            "0.6875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (4895 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18\n",
            "0.6953125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3330 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19\n",
            "0.75390625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3040 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "0.7604166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2299 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n",
            "0.8307291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1009 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n",
            "0.6393229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (893 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23\n",
            "0.76692706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2443 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n",
            "0.64453125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3009 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n",
            "0.78125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3159 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n",
            "0.70703125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (748 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27\n",
            "0.8268229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2729 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28\n",
            "0.7786459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2515 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29\n",
            "0.79296875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1546 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "0.70182294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3575 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31\n",
            "0.7135416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3116 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "0.8059896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2575 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33\n",
            "0.6380208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1222 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34\n",
            "0.7200521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2973 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35\n",
            "0.7291666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3127 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n",
            "0.7760416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1979 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37\n",
            "0.8138021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (629 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38\n",
            "0.67317706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2159 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39\n",
            "0.7630209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2395 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\n",
            "0.73828125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3029 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41\n",
            "0.78515625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_info.to_csv('/content/drive/MyDrive/AI_Project_14/MP4_GOOD/test_text.csv',index=False)"
      ],
      "metadata": {
        "id": "C2syjXV836Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_info['label']=1 #라벨링"
      ],
      "metadata": {
        "id": "a8JTa3lrKL0_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
