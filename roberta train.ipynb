{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":69387,"status":"ok","timestamp":1684696040970,"user":{"displayName":"김준서","userId":"08761208372709496633"},"user_tz":-540},"id":"-nfA_4g2y-RS","outputId":"7fb98e9e-e640-44c9-b684-e7aa9d2f8187"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["# set module\n","import pandas as pd\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k2qgnucDHCM2"},"outputs":[],"source":["## hugging face의 transformer 다운\n","# !pip install transformers==4.28.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uf6LU5RsLfIT"},"outputs":[],"source":["\"\"\"\n","주어진 오류는 Trainer와 PyTorch를 함께 사용할 때 accelerate 패키지가 필요하다는 내용입니다. accelerate는 Hugging Face의 Trainer와 PyTorch를 함께 사용할 때 모델 학습을 가속화하기 위해 설계된 패키지입니다.\n","\"\"\"\n","# !pip install git+https://github.com/huggingface/accelerate"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684696055173,"user":{"displayName":"김준서","userId":"08761208372709496633"},"user_tz":-540},"id":"QglLPgm7ZVeJ","outputId":"d5178410-8e30-47a0-d973-e7168cdf6b96"},"outputs":[{"name":"stdout","output_type":"stream","text":["현재 작업 디렉토리: /content/drive/MyDrive\n"]}],"source":["import os\n","\n","new_dir = \"/content/drive/MyDrive\"\n","os.chdir(new_dir)  # 현재 작업 디렉토리 변경\n","current_dir = os.getcwd()\n","print(\"현재 작업 디렉토리:\", current_dir)\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":369,"referenced_widgets":["2dd1f18c655c4b3dbbad1ec729c387eb","bbed9685f7324fea9e2c26f5616de1a4","89d3e8eaa6cc4972a41ddc7ac101da89","3e67b08eeef447cfa750e9c01ed5265b","db099da91cd64a9ca1a1db1853733578","7b017dd534304ec287d1c9420d903aad","68b7b3f5efc8414fb1eb6750f7be4606","f0e694858c7f48b38103264b984dfad0","c1a1c81a98ba42cb814b7bc7d739a7b7","49359da116c14d2aba02062dd3c8d0ad","c06c53275df1498d9ece3569ed42fbf3","8675310b8e0c4820942825c6e63261fb","2a4736c207c44420b214f53d3e4a6c6f","025b0ca2a8294ec08912e70d2e9ade2d","5f9ee3c01c7045cf829e5bc5054cf8a6","2b395ab489894e2cbaa8f9e86ae9cbe9","173e60583f5245c884839d83f7eb1502","ca6a52aba5524047bda01975a135c484","2f9b5bf595054aadb5db96e92a6a9615","8f7c7b1370124d559c9eb014f6be5097","db6ad857ca5a4e3eb8d99148d370be0c","163911d314c34133847717465458413f","4412ac5962c941fca32f1101238fcb8a","eaae2f890e7c4134b67fbd7adf06bd03","cfebdaa25e3548778418db37120cc0c4","73aa109f269546c1a7e82e265941aa06","32d9d620c628483baf9a57c71c785419","296279958edb44fba6d7ff38bcb74d90","f834cb1c39f64976a4fdd7d4d038685b","02a03ec207104998b26f9ce7d4c93db1","c229d1db28b54a18b1b07fb088d54615","c3f6fc6cc7874416b22e94503ff26792","cec7e8d6c1a64dc1b13466266fcbeb5d","a59be01f6bcc414c9a85613fe8f1285f","28ac832840074c15bdaca4c92963fdf2","304a18af9baf48d09b21bee8a209c489","3932c4032a4e463dbb6613a217c79060","af5180b1b7594a0cbaf2763661478576","f63d98e70b95431f89a509c566e96bc5","9fc6b3e0722e4cd2bde963e64fa04247","bb5c601f664d4d10b6e02775fb21ea3f","bb5e5c3801fd4e13ac96a97e19625bc6","5e2b0593023a42ba9a016c1cc3df96f4","b1df4b183d82436c8d8e6eb76882f317"]},"executionInfo":{"elapsed":18709,"status":"ok","timestamp":1684696095875,"user":{"displayName":"김준서","userId":"08761208372709496633"},"user_tz":-540},"id":"liq8NHj9Fdw-","outputId":"22a53465-6040-4a23-cb17-dc1e054d4944"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2dd1f18c655c4b3dbbad1ec729c387eb","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8675310b8e0c4820942825c6e63261fb","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4412ac5962c941fca32f1101238fcb8a","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a59be01f6bcc414c9a85613fe8f1285f","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","from transformers import Trainer\n","from transformers import TrainingArguments\n","from sklearn.metrics import accuracy_score\n","import torch\n","\n","# GPU를 사용하려면 다음 코드를 실행하세요.\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# RoBERTa 토크나이저와 모델을 불러옵니다.\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n","\n","class roberta_train():\n","  def __init__(self, path, filename):\n","    self.dataset = None\n","    self.path = path\n","    self.filename = filename\n","    self.train_dataset = None\n","\n","  # 데이터셋을 로드하고 토큰화합니다.\n","  def load_dataset(self):\n","      # TODO: 데이터셋을 로드하는 코드를 작성하세요.\n","      self.dataset = pd.read_csv(self.path + self.filename)\n","      return self.dataset\n"," \n","\n","  def tokenize_dataset(self):\n","      tokenized = tokenizer(self.dataset['script'].tolist(), padding='max_length', truncation=True, max_length=128)\n","      tokenized['label'] = self.dataset['label']\n","      self.train_dataset = tokenized\n","      return self.train_dataset\n","\n","\n","\n","# 데이터셋을 PyTorch의 Dataset 형식으로 변환합니다.\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hu9ZR6b0Ibgh"},"outputs":[],"source":["# train_dataset = Dataset(train_dataset, train_dataset.label)\n","\n","# # Trainer 및 TrainingArguments를 설정합니다.\n","# training_args = TrainingArguments(\n","#     output_dir='./results',\n","#     num_train_epochs=3,\n","#     per_device_train_batch_size=16,\n","#     per_device_eval_batch_size=16,\n","#     warmup_steps=500,\n","#     weight_decay=0.01,\n","#     logging_dir='./logs',\n","#     logging_steps=10\n","# )\n","\n","# # Trainer 객체를 생성하고 fine-tuning을 실행합니다.\n","# trainer = Trainer(\n","#     model=model,\n","#     args=training_args,\n","#     train_dataset=train_dataset,\n","#     compute_metrics=lambda pred: {'accuracy': accuracy_score(pred.label_ids, pred.predictions.argmax(-1))}\n","# )\n","\n","# trainer.train()\n","# from sklearn.model_selection import KFold\n","# train_dataset = Dataset(train_dataset, train_dataset.label)\n","\n","\n","# # 데이터셋을 K-fold로 나누기 위해 KFold 객체를 생성합니다.\n","# kfold = KFold(n_splits=5, shuffle=True)\n","\n","# # K-fold를 반복하여 학습을 수행합니다.\n","# for fold, (train_indices, val_indices) in enumerate(kfold.split(train_dataset)):\n","#     # 현재 fold에 해당하는 데이터를 추출합니다.\n","#     train_data_fold = [train_dataset[i] for i in train_indices]\n","#     val_data_fold = [train_dataset[i] for i in val_indices]\n","\n","#     # Trainer 및 TrainingArguments를 설정합니다.\n","#     training_args = TrainingArguments(\n","#         output_dir=f'./results_fold_{fold}',\n","#         num_train_epochs=3,\n","#         per_device_train_batch_size=16,\n","#         per_device_eval_batch_size=16,\n","#         warmup_steps=500,\n","#         weight_decay=0.01,\n","#         logging_dir=f'./logs_fold_{fold}',\n","#         logging_steps=10\n","#     )\n","\n","#     # Trainer 객체를 생성하고 fine-tuning을 실행합니다.\n","#     trainer = Trainer(\n","#         model=model,\n","#         args=training_args,\n","#         train_dataset=train_data_fold,\n","#         eval_dataset=val_data_fold,\n","#         compute_metrics=lambda pred: {'accuracy': accuracy_score(pred.label_ids, pred.predictions.argmax(-1))}\n","#     )\n","\n","#     trainer.train()\n","#     trainer.save_model(\"./saved_model\")  # 모델을 지정한 경로에 저장합니다.\n","\n","## cv 사용해서 이전보다 accuracy 높을 경우에 저장"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d2Ih9Sf4TW-8"},"outputs":[],"source":["# from sklearn.model_selection import KFold\n","# from sklearn.metrics import accuracy_score\n","# import os\n","\n","# train_dataset = Dataset(train_dataset, train_dataset.label)\n","\n","# # 데이터셋을 K-fold로 나누기 위해 KFold 객체를 생성합니다.\n","# kfold = KFold(n_splits=5, shuffle=True)\n","\n","# # K-fold를 반복하여 학습을 수행합니다.\n","# for fold, (train_indices, val_indices) in enumerate(kfold.split(train_dataset)):\n","#     # 현재 fold에 해당하는 데이터를 추출합니다.\n","#     train_data_fold = [train_dataset[i] for i in train_indices]\n","#     val_data_fold = [train_dataset[i] for i in val_indices]\n","\n","#     # Trainer 및 TrainingArguments를 설정합니다.\n","#     output_dir = f'./content/drive/MyDrive/bert_train_project/results_fold_{fold}'\n","#     logging_dir = f'./content/drive/MyDrive/bert_train_project/logs_fold_{fold}'\n","\n","#     training_args = TrainingArguments(\n","#         output_dir=output_dir,\n","#         num_train_epochs=30,\n","#         per_device_train_batch_size=16,\n","#         per_device_eval_batch_size=16,\n","#         warmup_steps=500,\n","#         weight_decay=0.01,\n","#         logging_dir=logging_dir,\n","#         logging_steps=10,\n","#         save_strategy='steps',  # 매 50개의 steps마다 모델을 저장하도록 설정합니다.\n","#         save_steps=50,\n","#         save_total_limit=10,  # 최근 10개의 모델만 유지하고 나머지는 자동으로 삭제합니다.\n","#         evaluation_strategy='steps',  # 매 50개의 steps마다 검증을 수행합니다.\n","#         eval_steps=50,\n","#         logging_first_step=True,  # 첫 번째 step에서도 로깅을 수행합니다.\n","#         disable_tqdm=True  # tqdm progress bar를 사용하지 않습니다.\n","#     )\n","\n","#     # EarlyStopping 콜백을 정의합니다.\n","#     early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=5)\n","\n","#     # Trainer 객체를 생성하고 fine-tuning을 실행합니다.\n","#     trainer = Trainer(\n","#         model=model,\n","#         args=training_args,\n","#         train_dataset=train_data_fold,\n","#         eval_dataset=val_data_fold,\n","#         compute_metrics=lambda pred: {'accuracy': accuracy_score(pred.label_ids, pred.predictions.argmax(-1))},\n","#         callbacks=[early_stopping_callback]  # EarlyStopping 콜백을 추가합니다.\n","#     )\n","\n","#     # fine-tuning을 실행하면서 모델을 저장\n","#     trainer.train()\n","\n","#     # 모델 저장을 위해 저장된 로그 파일 중에서 최근 50개만 선택합니다.\n","#     log_files = sorted(os.listdir(logging_dir))\n","#     selected_logs = log_files[-50:]\n","\n","#     # 최종 모델만 저장하고 이전 모델 및 로그는 삭제합니다.\n","#     os.makedirs(f\"./content/drive/MyDrive/bert_train_project/saved_model_fold_{fold}\", exist_ok=True)\n","#     for log in selected_logs:\n","#         shutil.move(os.path.join(logging_dir, log), os.path.join(output_dir, log))\n","#     trainer.save_model(f\"./content/drive/MyDrive/bert_train_project/saved_model_fold_{fold}/final_model\")\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":741671,"status":"ok","timestamp":1684696837537,"user":{"displayName":"김준서","userId":"08761208372709496633"},"user_tz":-540},"id":"EqVG8Y8Goa-g","outputId":"ae261dad-20c7-4871-9ae2-42f1412a74d4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.6803, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.09}\n","{'loss': 0.4341, 'learning_rate': 1e-05, 'epoch': 9.09}\n","{'eval_loss': 0.0019315374083817005, 'eval_accuracy': 1.0, 'eval_runtime': 0.2769, 'eval_samples_per_second': 155.3, 'eval_steps_per_second': 10.835, 'epoch': 9.09}\n","{'loss': 0.001, 'learning_rate': 2e-05, 'epoch': 18.18}\n","{'eval_loss': 0.00028080641641281545, 'eval_accuracy': 1.0, 'eval_runtime': 0.2896, 'eval_samples_per_second': 148.5, 'eval_steps_per_second': 10.36, 'epoch': 18.18}\n","{'loss': 0.0003, 'learning_rate': 3e-05, 'epoch': 27.27}\n","{'eval_loss': 0.00011179783177794889, 'eval_accuracy': 1.0, 'eval_runtime': 0.298, 'eval_samples_per_second': 144.31, 'eval_steps_per_second': 10.068, 'epoch': 27.27}\n","{'train_runtime': 139.9265, 'train_samples_per_second': 36.233, 'train_steps_per_second': 2.358, 'train_loss': 0.13271587479746702, 'epoch': 30.0}\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0002, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.09}\n","{'loss': 0.0111, 'learning_rate': 1e-05, 'epoch': 9.09}\n","{'eval_loss': 1.7296286387136206e-05, 'eval_accuracy': 1.0, 'eval_runtime': 0.2979, 'eval_samples_per_second': 144.364, 'eval_steps_per_second': 10.072, 'epoch': 9.09}\n","{'loss': 0.0009, 'learning_rate': 2e-05, 'epoch': 18.18}\n","{'eval_loss': 3.6649855701398337e-06, 'eval_accuracy': 1.0, 'eval_runtime': 0.2934, 'eval_samples_per_second': 146.533, 'eval_steps_per_second': 10.223, 'epoch': 18.18}\n","{'loss': 0.0, 'learning_rate': 3e-05, 'epoch': 27.27}\n","{'eval_loss': 1.5746700228191912e-06, 'eval_accuracy': 1.0, 'eval_runtime': 0.3005, 'eval_samples_per_second': 143.083, 'eval_steps_per_second': 9.983, 'epoch': 27.27}\n","{'train_runtime': 136.0331, 'train_samples_per_second': 37.27, 'train_steps_per_second': 2.426, 'train_loss': 0.0035838918283512943, 'epoch': 30.0}\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.09}\n","{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 9.09}\n","{'eval_loss': 4.825137693842407e-07, 'eval_accuracy': 1.0, 'eval_runtime': 0.2838, 'eval_samples_per_second': 147.993, 'eval_steps_per_second': 10.571, 'epoch': 9.09}\n","{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 18.18}\n","{'eval_loss': 1.2204759514133912e-07, 'eval_accuracy': 1.0, 'eval_runtime': 0.2921, 'eval_samples_per_second': 143.785, 'eval_steps_per_second': 10.27, 'epoch': 18.18}\n","{'loss': 0.0, 'learning_rate': 3e-05, 'epoch': 27.27}\n","{'eval_loss': 2.838316248698902e-09, 'eval_accuracy': 1.0, 'eval_runtime': 0.284, 'eval_samples_per_second': 147.884, 'eval_steps_per_second': 10.563, 'epoch': 27.27}\n","{'train_runtime': 138.2078, 'train_samples_per_second': 36.901, 'train_steps_per_second': 2.388, 'train_loss': 7.385011926861338e-07, 'epoch': 30.0}\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.09}\n","{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 9.09}\n","{'eval_loss': 0.0, 'eval_accuracy': 1.0, 'eval_runtime': 0.2849, 'eval_samples_per_second': 147.426, 'eval_steps_per_second': 10.53, 'epoch': 9.09}\n","{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 18.18}\n","{'eval_loss': 0.0, 'eval_accuracy': 1.0, 'eval_runtime': 0.2846, 'eval_samples_per_second': 147.595, 'eval_steps_per_second': 10.543, 'epoch': 18.18}\n","{'loss': 0.0, 'learning_rate': 3e-05, 'epoch': 27.27}\n","{'eval_loss': 0.0, 'eval_accuracy': 1.0, 'eval_runtime': 0.2946, 'eval_samples_per_second': 142.55, 'eval_steps_per_second': 10.182, 'epoch': 27.27}\n","{'train_runtime': 133.2894, 'train_samples_per_second': 38.263, 'train_steps_per_second': 2.476, 'train_loss': 2.967588743360645e-08, 'epoch': 30.0}\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.0, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.09}\n","{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 9.09}\n","{'eval_loss': 0.0, 'eval_accuracy': 1.0, 'eval_runtime': 0.2813, 'eval_samples_per_second': 149.305, 'eval_steps_per_second': 10.665, 'epoch': 9.09}\n","{'loss': 0.0, 'learning_rate': 2e-05, 'epoch': 18.18}\n","{'eval_loss': 0.0, 'eval_accuracy': 1.0, 'eval_runtime': 0.2916, 'eval_samples_per_second': 144.042, 'eval_steps_per_second': 10.289, 'epoch': 18.18}\n","{'loss': 0.0, 'learning_rate': 3e-05, 'epoch': 27.27}\n","{'eval_loss': 0.0, 'eval_accuracy': 1.0, 'eval_runtime': 0.2815, 'eval_samples_per_second': 149.175, 'eval_steps_per_second': 10.655, 'epoch': 27.27}\n","{'train_runtime': 145.3583, 'train_samples_per_second': 35.086, 'train_steps_per_second': 2.27, 'train_loss': 0.0, 'epoch': 30.0}\n"]}],"source":["from transformers import TrainerCallback\n","\n","## train_dataset 제작,\n","path = \"bert_train_project/\"\n","filename = \"train.csv\"\n","train_dat = roberta_train(path, filename)\n","show_data = train_dat.load_dataset()\n","train_dataset = train_dat.tokenize_dataset()\n","\n","\n","class EarlyStoppingCallback(TrainerCallback):\n","    def __init__(self, early_stopping_patience=5, metric_name=\"eval_loss\"):\n","        self.early_stopping_patience = early_stopping_patience\n","        self.metric_name = metric_name\n","        self.best_metric = float(\"inf\")\n","        self.counter = 0\n","\n","    def on_evaluate(self, args, state, control, **kwargs):\n","        current_metric = state.log_history[-1].get(self.metric_name)\n","        if current_metric is not None and current_metric < self.best_metric:\n","            self.best_metric = current_metric\n","            self.counter = 0\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.early_stopping_patience:\n","                control.should_training_stop = True\n","\n","\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score\n","import os\n","import shutil\n","\n","train_dataset = Dataset(train_dataset, train_dataset.label)\n","\n","# 데이터셋을 K-fold로 나누기 위해 KFold 객체를 생성합니다.\n","kfold = KFold(n_splits=5, shuffle=True)\n","\n","# K-fold를 반복하여 학습을 수행합니다.\n","for fold, (train_indices, val_indices) in enumerate(kfold.split(train_dataset)):\n","    # 현재 fold에 해당하는 데이터를 추출합니다.\n","    train_data_fold = [train_dataset[i] for i in train_indices]\n","    val_data_fold = [train_dataset[i] for i in val_indices]\n","\n","    # Trainer 및 TrainingArguments를 설정합니다.\n","    output_dir = f'bert_train_project/results_fold_{fold}'\n","    logging_dir = f'bert_train_project/logs_fold_{fold}'\n","\n","    training_args = TrainingArguments(\n","        output_dir=output_dir,\n","        num_train_epochs=30,\n","        per_device_train_batch_size=16,\n","        per_device_eval_batch_size=16,\n","        warmup_steps=500,\n","        weight_decay=0.01,\n","        logging_dir=logging_dir,\n","        logging_steps=100,\n","        save_strategy='steps',  # 매 100개의 steps마다 모델을 저장하도록 설정합니다.\n","        save_steps=100,\n","        save_total_limit=10,  # 최근 10개의 모델만 유지하고 나머지는 자동으로 삭제합니다.\n","        evaluation_strategy='steps',  # 매 50개의 steps마다 검증을 수행합니다.\n","        eval_steps=100,\n","        logging_first_step=True,  # 첫 번째 step에서도 로깅을 수행합니다.\n","        disable_tqdm=True  # tqdm progress bar를 사용하지 않습니다.\n","    )\n","\n","    # EarlyStopping 콜백을 정의합니다.\n","    early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=5)\n","\n","    # Trainer 객체를 생성하고 fine-tuning을 실행합니다.\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_data_fold,\n","        eval_dataset=val_data_fold,\n","        compute_metrics=lambda pred: {'accuracy': accuracy_score(pred.label_ids, pred.predictions.argmax(-1))},\n","        callbacks=[early_stopping_callback]  # EarlyStopping 콜백을 추가합니다.\n","    )\n","\n","    # fine-tuning을 실행하면서 모델을 저장\n","    trainer.train()\n","\n","    # 모델 저장을 위해 저장된 로그 파일 중에서 최근 50개만 선택합니다.\n","    log_files = sorted(os.listdir(logging_dir))\n","    selected_logs = log_files[-50:]\n","\n","    # 최종 모델만 저장하고 이전 모델 및 로그는 삭제합니다.\n","    os.makedirs(f\"saved_model_fold_{fold}\", exist_ok=True)\n","    for log in selected_logs:\n","        shutil.move(os.path.join(logging_dir, log), os.path.join(output_dir, log))\n","    trainer.save_model(f\"saved_model_fold_{fold}/final_model\")\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":10766,"status":"ok","timestamp":1684697152044,"user":{"displayName":"김준서","userId":"08761208372709496633"},"user_tz":-540},"id":"bJQk9SV5xOmI"},"outputs":[],"source":["saved_model_path = \"saved_model_fold_1/final_model\"\n","\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","my_model = RobertaForSequenceClassification.from_pretrained(saved_model_path)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":5995,"status":"ok","timestamp":1684697519279,"user":{"displayName":"김준서","userId":"08761208372709496633"},"user_tz":-540},"id":"kEnBGuOmxlHJ"},"outputs":[],"source":["# from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","\n","# # Load the tokenizer\n","# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","\n","# # Load your fine-tuned model\n","# saved_model_path = \"saved_model_fold_1/final_model\"\n","# my_model = RobertaForSequenceClassification.from_pretrained(saved_model_path)\n","# # Example text\n","# text = \"\"\"Good evening, ladies and gentlemen! Thank you for being here tonight. I want to talk to you about something that is at the core of our existence: human connection. It's a topic that has fascinated me for years, and I believe it holds the key to our personal growth, happiness, and collective success.\n","# Let me start with a story. Imagine a bustling city, where people pass each other on the streets without a second glance. Now, picture a different scenario: a small town where neighbors greet each other warmly, share stories, and lend a helping hand. Which community do you think is happier and more vibrant? The latter, of course. Why? Because human connection matters.\n","# So, what exactly is human connection? It's more than just casual interactions or digital friendships. Human connection is about forging meaningful relationships, understanding and empathizing with one another, and experiencing a sense of belonging. It's about creating bonds that transcend differences and make us feel alive. Now, let's explore the impact of human connection on our lives. Research shows that people with strong social connections tend to live longer, healthier lives. When we feel connected, our stress levels decrease, our immune systems strengthen, and we become more resilient in the face of challenges. Human connection is the secret ingredient that nourishes our well-being.\n","# In today's digital age, it's easy to get caught up in virtual interactions and forget the power of genuine human connection. While technology has brought us closer in many ways, it has also created a sense of disconnection. We may have thousands of online friends, but how many true connections do we have? It's time to prioritize quality over quantity.\n","# Now, the question arises: how can we build genuine connections in a world that often feels disconnected? It starts with being present and actively listening to others. We need to engage in meaningful conversations, ask open-ended questions, and show genuine interest in others' stories. It's about stepping outside our comfort zones and embracing vulnerability. When we prioritize human connection, something incredible happens: a ripple effect. Our positive interactions with others inspire them to do the same. We create a chain reaction of compassion, kindness, and understanding that spreads far beyond our immediate circle. Imagine the transformative power of millions of interconnected individuals making a difference.\n","# In conclusion, let's remember that we are wired for connection. It's in our DNA. By fostering genuine human connections, we can unlock our full potential and create a more compassionate, inclusive world. So, let's reach out, connect, and make a difference in each other's lives. Together, we can change the world.\"\"\"\n","\n","# # Tokenize the input text\n","# encoded_inputs = tokenizer.encode_plus(text, padding=True, truncation=True, return_tensors='pt')\n","\n","# # Forward pass through the model\n","# outputs = my_model(**encoded_inputs)\n","\n","# # Retrieve the predicted logits for NSP (next sentence prediction)\n","# nsp_logits = outputs.logits\n","\n","# # Apply softmax to obtain the predicted probabilities\n","# import torch.nn.functional as F\n","# nsp_probabilities = F.softmax(nsp_logits, dim=1)\n","\n","# # The shape of nsp_probabilities is (1, 2)\n","# # The first element in the second dimension represents the probability of the next sentence being a continuation\n","# # The second element represents the probability of the next sentence being randomly selected from the training data\n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":10492,"status":"ok","timestamp":1684698335013,"user":{"displayName":"김준서","userId":"08761208372709496633"},"user_tz":-540},"id":"DW9aeUcjzD1b"},"outputs":[],"source":["import torch\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","import re\n","\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","# Load your fine-tuned model\n","saved_model_path = \"saved_model_fold_1/final_model\"\n","my_model = RobertaForSequenceClassification.from_pretrained(saved_model_path)\n","\n","def get_nsp_prob(sentence_1, sentence_2):\n","    # 문장을 토큰화하고 인코딩합니다.\n","    inputs = tokenizer.encode_plus(\n","        sentence_1, \n","        sentence_2, \n","        add_special_tokens=True, \n","        return_tensors='pt'\n","    )\n","\n","    # 모델에 입력을 전달하고 출력을 가져옵니다.\n","    outputs = my_model(\n","        input_ids=inputs['input_ids'], \n","        attention_mask=inputs['attention_mask']\n","    )\n","\n","    # \"이어지는 문장\"에 대한 확률을 반환합니다.\n","    logits = outputs.logits\n","    nsp_prob = torch.softmax(logits, dim=1)\n","    return nsp_prob[:, 1].item()\n","\n","input_file = './bert_train_project/test/input.txt'\n","output_file = './bert_train_project/test/output.txt'\n","\n","# 입력 파일에서 문장 쌍을 읽어들입니다.\n","with open(input_file, 'r') as f:\n","    lines = f.readlines()\n","\n","# 출력 파일을 생성합니다.\n","with open(output_file, 'w') as f:\n","    # 각 문장 쌍에 대해 NSP를 계산하고 출력 파일에 기록합니다.\n","    for line in lines:\n","        # 문장을 마침표나 느낌표로 구분해서 두 개의 문장으로 나눕니다.\n","        try:\n","            sentences = re.split('[.!?]', line.strip())\n","            sentence_1 = sentences[0].strip()\n","            sentence_2 = sentences[1].strip()\n","        except:\n","            pass\n","\n","        # NSP를 계산하고 출력 파일에 기록합니다.\n","        nsp_prob = get_nsp_prob(sentence_1, sentence_2)\n","        f.write(f'{nsp_prob}\\n')\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO9uTTEqV1ah+Xigw9mMCfg","gpuType":"T4","mount_file_id":"1MUkOqIjPowp9qOxX9lmoTEA57SctYxp1","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"025b0ca2a8294ec08912e70d2e9ade2d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f9b5bf595054aadb5db96e92a6a9615","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8f7c7b1370124d559c9eb014f6be5097","value":456318}},"02a03ec207104998b26f9ce7d4c93db1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"163911d314c34133847717465458413f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"173e60583f5245c884839d83f7eb1502":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28ac832840074c15bdaca4c92963fdf2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f63d98e70b95431f89a509c566e96bc5","placeholder":"​","style":"IPY_MODEL_9fc6b3e0722e4cd2bde963e64fa04247","value":"Downloading pytorch_model.bin: 100%"}},"296279958edb44fba6d7ff38bcb74d90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a4736c207c44420b214f53d3e4a6c6f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_173e60583f5245c884839d83f7eb1502","placeholder":"​","style":"IPY_MODEL_ca6a52aba5524047bda01975a135c484","value":"Downloading (…)olve/main/merges.txt: 100%"}},"2b395ab489894e2cbaa8f9e86ae9cbe9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dd1f18c655c4b3dbbad1ec729c387eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bbed9685f7324fea9e2c26f5616de1a4","IPY_MODEL_89d3e8eaa6cc4972a41ddc7ac101da89","IPY_MODEL_3e67b08eeef447cfa750e9c01ed5265b"],"layout":"IPY_MODEL_db099da91cd64a9ca1a1db1853733578"}},"2f9b5bf595054aadb5db96e92a6a9615":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"304a18af9baf48d09b21bee8a209c489":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb5c601f664d4d10b6e02775fb21ea3f","max":501200538,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bb5e5c3801fd4e13ac96a97e19625bc6","value":501200538}},"32d9d620c628483baf9a57c71c785419":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3932c4032a4e463dbb6613a217c79060":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e2b0593023a42ba9a016c1cc3df96f4","placeholder":"​","style":"IPY_MODEL_b1df4b183d82436c8d8e6eb76882f317","value":" 501M/501M [00:02&lt;00:00, 230MB/s]"}},"3e67b08eeef447cfa750e9c01ed5265b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49359da116c14d2aba02062dd3c8d0ad","placeholder":"​","style":"IPY_MODEL_c06c53275df1498d9ece3569ed42fbf3","value":" 899k/899k [00:00&lt;00:00, 1.07MB/s]"}},"4412ac5962c941fca32f1101238fcb8a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eaae2f890e7c4134b67fbd7adf06bd03","IPY_MODEL_cfebdaa25e3548778418db37120cc0c4","IPY_MODEL_73aa109f269546c1a7e82e265941aa06"],"layout":"IPY_MODEL_32d9d620c628483baf9a57c71c785419"}},"49359da116c14d2aba02062dd3c8d0ad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e2b0593023a42ba9a016c1cc3df96f4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f9ee3c01c7045cf829e5bc5054cf8a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db6ad857ca5a4e3eb8d99148d370be0c","placeholder":"​","style":"IPY_MODEL_163911d314c34133847717465458413f","value":" 456k/456k [00:00&lt;00:00, 11.2MB/s]"}},"68b7b3f5efc8414fb1eb6750f7be4606":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"73aa109f269546c1a7e82e265941aa06":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3f6fc6cc7874416b22e94503ff26792","placeholder":"​","style":"IPY_MODEL_cec7e8d6c1a64dc1b13466266fcbeb5d","value":" 481/481 [00:00&lt;00:00, 11.9kB/s]"}},"7b017dd534304ec287d1c9420d903aad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8675310b8e0c4820942825c6e63261fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2a4736c207c44420b214f53d3e4a6c6f","IPY_MODEL_025b0ca2a8294ec08912e70d2e9ade2d","IPY_MODEL_5f9ee3c01c7045cf829e5bc5054cf8a6"],"layout":"IPY_MODEL_2b395ab489894e2cbaa8f9e86ae9cbe9"}},"89d3e8eaa6cc4972a41ddc7ac101da89":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0e694858c7f48b38103264b984dfad0","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c1a1c81a98ba42cb814b7bc7d739a7b7","value":898823}},"8f7c7b1370124d559c9eb014f6be5097":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9fc6b3e0722e4cd2bde963e64fa04247":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a59be01f6bcc414c9a85613fe8f1285f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_28ac832840074c15bdaca4c92963fdf2","IPY_MODEL_304a18af9baf48d09b21bee8a209c489","IPY_MODEL_3932c4032a4e463dbb6613a217c79060"],"layout":"IPY_MODEL_af5180b1b7594a0cbaf2763661478576"}},"af5180b1b7594a0cbaf2763661478576":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1df4b183d82436c8d8e6eb76882f317":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb5c601f664d4d10b6e02775fb21ea3f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb5e5c3801fd4e13ac96a97e19625bc6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bbed9685f7324fea9e2c26f5616de1a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b017dd534304ec287d1c9420d903aad","placeholder":"​","style":"IPY_MODEL_68b7b3f5efc8414fb1eb6750f7be4606","value":"Downloading (…)olve/main/vocab.json: 100%"}},"c06c53275df1498d9ece3569ed42fbf3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1a1c81a98ba42cb814b7bc7d739a7b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c229d1db28b54a18b1b07fb088d54615":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c3f6fc6cc7874416b22e94503ff26792":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca6a52aba5524047bda01975a135c484":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cec7e8d6c1a64dc1b13466266fcbeb5d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfebdaa25e3548778418db37120cc0c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_02a03ec207104998b26f9ce7d4c93db1","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c229d1db28b54a18b1b07fb088d54615","value":481}},"db099da91cd64a9ca1a1db1853733578":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db6ad857ca5a4e3eb8d99148d370be0c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eaae2f890e7c4134b67fbd7adf06bd03":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_296279958edb44fba6d7ff38bcb74d90","placeholder":"​","style":"IPY_MODEL_f834cb1c39f64976a4fdd7d4d038685b","value":"Downloading (…)lve/main/config.json: 100%"}},"f0e694858c7f48b38103264b984dfad0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f63d98e70b95431f89a509c566e96bc5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f834cb1c39f64976a4fdd7d4d038685b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
