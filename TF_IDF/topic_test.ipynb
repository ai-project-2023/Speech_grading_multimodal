{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPJPqIUh84EoPOtPSFA/kRR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hTsSGV-9x_Hz","executionInfo":{"status":"ok","timestamp":1685379899238,"user_tz":-540,"elapsed":21347,"user":{"displayName":"눈돌","userId":"07216448601076797764"}},"outputId":"b8944994-0d97-423b-cc89-fbb08d770c4f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from sklearn.linear_model import LogisticRegression\n","import joblib\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","loaded_model = joblib.load('/content/drive/MyDrive/logistic_model.pkl')"]},{"cell_type":"code","source":["import glob\n","\n","test_data_path = \"/content/drive/MyDrive/AI_Project_14/MP4_GOOD/text_results/\"\n","\n","file_list = []\n","file_contents = []\n","\n","for file in glob.glob(test_data_path + \"/*.txt\"):\n","    file_list.append(file)\n","    with open(file, 'r') as f:\n","        content = f.read()\n","        file_contents.append(content)"],"metadata":{"id":"KIqb2w1WyNap","executionInfo":{"status":"ok","timestamp":1685379982880,"user_tz":-540,"elapsed":31764,"user":{"displayName":"눈돌","userId":"07216448601076797764"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["print(len(file_list))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZzR7_NCcdktq","executionInfo":{"status":"ok","timestamp":1685379992326,"user_tz":-540,"elapsed":289,"user":{"displayName":"눈돌","userId":"07216448601076797764"}},"outputId":"4da598fa-a7f0-4e1d-adb5-ea4956d2d9c5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["89\n"]}]},{"cell_type":"code","source":["from gensim.parsing.preprocessing import preprocess_string\n","\n","pre_docs = []\n","n_docs = []\n","\n","for content in file_contents:\n","    # Preprocess the script text using the gensim preprocessing function\n","    pre_docs.append(\" \".join(preprocess_string(content)))\n","\n","    # Alternatively, if you don't want to preprocess the script text, you can use the following lines:\n","    n_docs.append(content)"],"metadata":{"id":"GvSmFFWpyPmI","executionInfo":{"status":"ok","timestamp":1685379998207,"user_tz":-540,"elapsed":1502,"user":{"displayName":"눈돌","userId":"07216448601076797764"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","import numpy as np\n","\n","# Initialize CountVectorizer\n","vect = CountVectorizer()  # You can specify the vocabulary using 'vocabulary=vocab' or leave it unspecified\n","\n","# Transform the document collection into a document-term matrix\n","dtm = vect.fit_transform(pre_docs).toarray()\n","\n","# Get the vocabulary (feature names)\n","vocab = vect.get_feature_names_out()\n","\n","# Initialize TfidfVectorizer with specified vocabulary\n","tfidfv = TfidfVectorizer(vocabulary=vocab)\n","\n","# Transform the document collection into a TF-IDF matrix\n","tfidf = tfidfv.fit_transform(pre_docs).toarray()\n","\n","# Get the feature names (vocabulary)\n","vocab = tfidfv.get_feature_names_out()\n","\n","# Create an array of the indices that would sort each row of the TF-IDF matrix in descending order\n","tfidf_order = np.flip(np.argsort(tfidf, axis=-1), axis=-1)\n","\n","topics = []  # List to store the top terms for each document\n","n_top = 5  # Number of top terms to retrieve for each document\n","\n","# Iterate over each row in tfidf_order\n","for line in tfidf_order:\n","    # Retrieve the top n_top terms for the current document\n","    topic = [vocab[x] for x in line[:n_top]]\n","    topics.append(topic)\n","\n","# Create a pandas DataFrame to display the top terms for each document\n","df = pd.DataFrame(topics)"],"metadata":{"id":"kk01wtTNyR9p","executionInfo":{"status":"ok","timestamp":1685380004120,"user_tz":-540,"elapsed":755,"user":{"displayName":"눈돌","userId":"07216448601076797764"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["%%capture\n","!pip install transformers\n","from transformers import pipeline\n","\n","classifier = pipeline('zero-shot-classification', model='roberta-large-mnli')"],"metadata":{"id":"FAl3LW2WyTO0","executionInfo":{"status":"ok","timestamp":1685380047941,"user_tz":-540,"elapsed":40573,"user":{"displayName":"눈돌","userId":"07216448601076797764"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["a = []  # List to store the indices\n","b = []  # List to store the scores\n","c = []  # List to store the counts\n","\n","for idx, (_) in enumerate(n_docs):\n","    sequence_to_classify = n_docs[idx]\n","    candidate_labels = ' '.join(df.iloc[idx][:3])\n","    score = classifier(sequence_to_classify, candidate_labels)['scores'][0]\n","\n","    words = sequence_to_classify.split()\n","    word_count = len(words)\n","\n","    idx += 1\n","\n","    a.append(idx)\n","    b.append(score)\n","    c.append(word_count)"],"metadata":{"id":"Bh7GYKS4yhgo","executionInfo":{"status":"ok","timestamp":1685380672809,"user_tz":-540,"elapsed":598331,"user":{"displayName":"눈돌","userId":"07216448601076797764"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import csv\n","import os\n","\n","test_data = list(zip(a, b, c))\n","test_data_path = '/content/drive/MyDrive/my_test_data.csv'\n","\n","if os.path.exists(test_data_path):\n","    os.remove(test_data_path)\n","\n","with open(test_data_path, 'w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow(['idx', 'score', 'count'])  # 첫 번째 행에 열 이름 쓰기\n","    writer.writerows(test_data)  # 데이터 쓰기"],"metadata":{"id":"gCsJB6LJy38C","executionInfo":{"status":"ok","timestamp":1685380679228,"user_tz":-540,"elapsed":266,"user":{"displayName":"눈돌","userId":"07216448601076797764"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["tdata = pd.read_csv(test_data_path)\n","\n","X = tdata[['score', 'count']]  # vector?\n","print(X)\n","\n","pred = loaded_model.predict(X)  # result\n","print(\"Prediction:\", pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BZLfA3ZUy8q_","executionInfo":{"status":"ok","timestamp":1685380682826,"user_tz":-540,"elapsed":428,"user":{"displayName":"눈돌","userId":"07216448601076797764"}},"outputId":"afe75a0a-56f1-4d50-f5f1-5d4eec2861f6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["       score  count\n","0   0.592899   3004\n","1   0.804751   1530\n","2   0.530126   1309\n","3   0.232205   1569\n","4   0.476543   1834\n","..       ...    ...\n","84  0.443273    815\n","85  0.473831   2715\n","86  0.413845    886\n","87  0.792928   1892\n","88  0.776828   2132\n","\n","[89 rows x 2 columns]\n","Prediction: [1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"]}]}]}