{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO231owrj6U2gSLLG6tOsUZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hTsSGV-9x_Hz","executionInfo":{"status":"ok","timestamp":1685353703814,"user_tz":-540,"elapsed":2774,"user":{"displayName":"눈돌","userId":"07216448601076797764"}},"outputId":"43bb9a06-e5c3-48b1-bec2-906b0e7cc1e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from sklearn.linear_model import LogisticRegression\n","import joblib\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","loaded_model = joblib.load('/content/drive/MyDrive/logistic_model.pkl')\n","\n","pre_docs= []\n","n_docs= []"]},{"cell_type":"code","source":["test_data_path = \"/content/drive/MyDrive/AI_Project_14/MP4_GOOD/text_results/\"\n","test_data_path += \"0000.txt\"\n","\n","with open(test_data_path, 'r', newline='') as file:\n","    content = file.read()"],"metadata":{"id":"KIqb2w1WyNap","executionInfo":{"status":"ok","timestamp":1685353706816,"user_tz":-540,"elapsed":444,"user":{"displayName":"눈돌","userId":"07216448601076797764"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["from gensim.parsing.preprocessing import preprocess_string\n","\n","# Preprocess the script text using the gensim preprocessing function\n","pre_docs.append(\" \".join(preprocess_string(content)))\n","\n","# Alternatively, if you don't want to preprocess the script text, you can use the following lines:\n","n_docs.append(content)"],"metadata":{"id":"GvSmFFWpyPmI","executionInfo":{"status":"ok","timestamp":1685353712240,"user_tz":-540,"elapsed":423,"user":{"displayName":"눈돌","userId":"07216448601076797764"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","import numpy as np\n","\n","# Initialize CountVectorizer\n","vect = CountVectorizer()  # You can specify the vocabulary using 'vocabulary=vocab' or leave it unspecified\n","\n","# Transform the document collection into a document-term matrix\n","dtm = vect.fit_transform(pre_docs).toarray()\n","\n","# Get the vocabulary (feature names)\n","vocab = vect.get_feature_names_out()\n","\n","# Initialize TfidfVectorizer with specified vocabulary\n","tfidfv = TfidfVectorizer(vocabulary=vocab)\n","\n","# Transform the document collection into a TF-IDF matrix\n","tfidf = tfidfv.fit_transform(pre_docs).toarray()\n","\n","# Get the feature names (vocabulary)\n","vocab = tfidfv.get_feature_names_out()\n","\n","# Create an array of the indices that would sort each row of the TF-IDF matrix in descending order\n","tfidf_order = np.flip(np.argsort(tfidf, axis=-1), axis=-1)\n","\n","topics = []  # List to store the top terms for each document\n","n_top = 5  # Number of top terms to retrieve for each document\n","\n","# Iterate over each row in tfidf_order\n","for line in tfidf_order:\n","    # Retrieve the top n_top terms for the current document\n","    topic = [vocab[x] for x in line[:n_top]]\n","    topics.append(topic)\n","\n","# Create a pandas DataFrame to display the top terms for each document\n","df = pd.DataFrame(topics)"],"metadata":{"id":"kk01wtTNyR9p","executionInfo":{"status":"ok","timestamp":1685353714449,"user_tz":-540,"elapsed":594,"user":{"displayName":"눈돌","userId":"07216448601076797764"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["%%capture\n","!pip install transformers\n","from transformers import pipeline\n","\n","classifier = pipeline('zero-shot-classification', model='roberta-large-mnli')"],"metadata":{"id":"FAl3LW2WyTO0","executionInfo":{"status":"ok","timestamp":1685351690826,"user_tz":-540,"elapsed":44051,"user":{"displayName":"눈돌","userId":"07216448601076797764"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["a = []  # List to store the indices\n","b = []  # List to store the scores\n","c = []  # List to store the counts\n","\n","for idx, (_) in enumerate(n_docs):\n","    sequence_to_classify = n_docs[idx]\n","    candidate_labels = ' '.join(df.iloc[idx][:3])\n","    score = classifier(sequence_to_classify, candidate_labels)['scores'][0]\n","\n","    words = sequence_to_classify.split()\n","    word_count = len(words)\n","\n","    idx += 1\n","\n","    a.append(idx)\n","    b.append(score)\n","    c.append(word_count)"],"metadata":{"id":"Bh7GYKS4yhgo","executionInfo":{"status":"ok","timestamp":1685353724474,"user_tz":-540,"elapsed":7602,"user":{"displayName":"눈돌","userId":"07216448601076797764"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["import csv\n","import os\n","\n","test_data = list(zip(a, b, c))\n","test_data_path = '/content/drive/MyDrive/my_test_data.csv'\n","\n","if os.path.exists(test_data_path):\n","    os.remove(test_data_path)\n","\n","with open(test_data_path, 'w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow(['idx', 'score', 'count'])  # 첫 번째 행에 열 이름 쓰기\n","    writer.writerows(test_data)  # 데이터 쓰기"],"metadata":{"id":"gCsJB6LJy38C","executionInfo":{"status":"ok","timestamp":1685353921969,"user_tz":-540,"elapsed":362,"user":{"displayName":"눈돌","userId":"07216448601076797764"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["tdata = pd.read_csv(test_data_path)\n","\n","X = tdata[['score', 'count']]  # vector?\n","print(X)\n","\n","pred = loaded_model.predict(X)  # result\n","print(\"Prediction:\", pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BZLfA3ZUy8q_","executionInfo":{"status":"ok","timestamp":1685353935761,"user_tz":-540,"elapsed":467,"user":{"displayName":"눈돌","userId":"07216448601076797764"}},"outputId":"3f26d6d5-8ffa-4423-af92-8eb2f341eaf5"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["      score  count\n","0  0.122714   3004\n","Prediction: [1]\n"]}]}]}