{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1BhQ5AqnJsdeCRsIWBQxk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ai-project-2023/Speech_grading_multimodal/blob/nlp%2Fspeechtotext/Bert_Cosine_Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bert"
      ],
      "metadata": {
        "id": "ogaZm4b-rwjl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iuWI34YrjHp"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "good_audio_info = pd.read_csv('file_path')\n",
        "bad_audio_info = pd.read_csv('file_path')"
      ],
      "metadata": {
        "id": "Yn-NNq5Er1Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "for i in range(good_audio_info.shape[0]):\n",
        "\n",
        "  model_name = 'bert-base-uncased'\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModel.from_pretrained(model_name)\n",
        "  cos_sim_list = []\n",
        "  # 문장 입력\n",
        "  sentence1 = good_audio_info.loc[i]['speech_to_text']\n",
        "  sentence2 = good_audio_info.loc[i]['script']\n",
        "\n",
        "  # 토큰화 및 임베딩 벡터 변환\n",
        "  tokens1 = tokenizer.tokenize(sentence1)\n",
        "  tokens2 = tokenizer.tokenize(sentence2)\n",
        "  input_ids1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
        "  input_ids2 = tokenizer.convert_tokens_to_ids(tokens2)\n",
        "  input_ids1_1 = input_ids1[:512]\n",
        "  input_ids2_1 = input_ids2[:512]\n",
        "\n",
        "  input_tensor1_1 = torch.tensor([input_ids1_1])\n",
        "  input_tensor2_1 = torch.tensor([input_ids2_1])\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output1_1 = model(input_tensor1_1)\n",
        "      output2_1 = model(input_tensor2_1)\n",
        "\n",
        "  sentence_embedding1_1 = torch.mean(output1_1[0], dim=1)  # 첫 번째 문장의 평균 임베딩\n",
        "  sentence_embedding2_1 = torch.mean(output2_1[0], dim=1)  # 두 번째 문장의 평균 임베딩\n",
        "\n",
        "  # cosine_similarity 계산\n",
        "  cosine_similarity_1 = torch.nn.functional.cosine_similarity(sentence_embedding1_1, sentence_embedding2_1, dim=0)\n",
        "\n",
        "  mean_value_1 = torch.mean(cosine_similarity_1)\n",
        "  cos_sim_list.append(mean_value_1)\n",
        "\n",
        "  input_ids1_2 = input_ids1[-512:]\n",
        "  input_ids2_2 = input_ids2[-512:]\n",
        "\n",
        "  input_tensor1_2 = torch.tensor([input_ids1_2])\n",
        "  input_tensor2_2 = torch.tensor([input_ids2_2])\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output1_2 = model(input_tensor1_2)\n",
        "      output2_2 = model(input_tensor2_2)\n",
        "\n",
        "  sentence_embedding1_2 = torch.mean(output1_2[0], dim=1)  # 첫 번째 문장의 평균 임베딩\n",
        "  sentence_embedding2_2 = torch.mean(output2_2[0], dim=1)  # 두 번째 문장의 평균 임베딩\n",
        "\n",
        "  # cosine_similarity 계산\n",
        "  cosine_similarity_2 = torch.nn.functional.cosine_similarity(sentence_embedding1_2, sentence_embedding2_2, dim=0)\n",
        "\n",
        "  mean_value_2 = torch.mean(cosine_similarity_2)\n",
        "  cos_sim_list.append(mean_value_2)\n",
        "  cos_sim = np.mean(cos_sim_list)\n",
        "  print(i)\n",
        "  print(cos_sim)\n",
        "  good_audio_info['cos_sim'].iloc[i] = cos_sim"
      ],
      "metadata": {
        "id": "NtTrwQYBsHK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "good_audio_info.to_csv('path', index=False)"
      ],
      "metadata": {
        "id": "z3khEBYhsNad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "for i in range(bad_audio_info.shape[0]):\n",
        "\n",
        "  model_name = 'bert-base-uncased'\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModel.from_pretrained(model_name)\n",
        "  cos_sim_list = []\n",
        "  # 문장 입력\n",
        "  sentence1 = bad_audio_info.loc[i]['speech_to_text']\n",
        "  sentence2 = bad_audio_info.loc[i]['script']\n",
        "\n",
        "  # 토큰화 및 임베딩 벡터 변환\n",
        "  tokens1 = tokenizer.tokenize(sentence1)\n",
        "  tokens2 = tokenizer.tokenize(sentence2)\n",
        "  input_ids1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
        "  input_ids2 = tokenizer.convert_tokens_to_ids(tokens2)\n",
        "  input_ids1_1 = input_ids1[:512]\n",
        "  input_ids2_1 = input_ids2[:512]\n",
        "\n",
        "  input_tensor1_1 = torch.tensor([input_ids1_1])\n",
        "  input_tensor2_1 = torch.tensor([input_ids2_1])\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output1_1 = model(input_tensor1_1)\n",
        "      output2_1 = model(input_tensor2_1)\n",
        "\n",
        "  sentence_embedding1_1 = torch.mean(output1_1[0], dim=1)  # 첫 번째 문장의 평균 임베딩\n",
        "  sentence_embedding2_1 = torch.mean(output2_1[0], dim=1)  # 두 번째 문장의 평균 임베딩\n",
        "\n",
        "  # cosine_similarity 계산\n",
        "  cosine_similarity_1 = torch.nn.functional.cosine_similarity(sentence_embedding1_1, sentence_embedding2_1, dim=0)\n",
        "\n",
        "  mean_value_1 = torch.mean(cosine_similarity_1)\n",
        "  cos_sim_list.append(mean_value_1)\n",
        "\n",
        "  input_ids1_2 = input_ids1[-512:]\n",
        "  input_ids2_2 = input_ids2[-512:]\n",
        "\n",
        "  input_tensor1_2 = torch.tensor([input_ids1_2])\n",
        "  input_tensor2_2 = torch.tensor([input_ids2_2])\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output1_2 = model(input_tensor1_2)\n",
        "      output2_2 = model(input_tensor2_2)\n",
        "\n",
        "  sentence_embedding1_2 = torch.mean(output1_2[0], dim=1)  # 첫 번째 문장의 평균 임베딩\n",
        "  sentence_embedding2_2 = torch.mean(output2_2[0], dim=1)  # 두 번째 문장의 평균 임베딩\n",
        "\n",
        "  # cosine_similarity 계산\n",
        "  cosine_similarity_2 = torch.nn.functional.cosine_similarity(sentence_embedding1_2, sentence_embedding2_2, dim=0)\n",
        "\n",
        "  mean_value_2 = torch.mean(cosine_similarity_2)\n",
        "  cos_sim_list.append(mean_value_2)\n",
        "  cos_sim = np.mean(cos_sim_list)\n",
        "  print(i)\n",
        "  print(cos_sim)\n",
        "  bad_audio_info['cos_sim'].iloc[i] = cos_sim"
      ],
      "metadata": {
        "id": "orm1cJ3Lr6ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad_audio_info.to_csv('path', index=False)"
      ],
      "metadata": {
        "id": "w2Jjqa9mr_N1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}